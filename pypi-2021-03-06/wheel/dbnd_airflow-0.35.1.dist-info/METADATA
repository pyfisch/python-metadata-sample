Metadata-Version: 2.1
Name: dbnd-airflow
Version: 0.35.1
Summary: Machine Learning Orchestration
Home-page: https://github.com/databand-ai/dbnd
Author: Evgeny Shulman
Author-email: evgeny.shulman@databand.ai
Maintainer: Evgeny Shulman
Maintainer-email: evgeny.shulman@databand.ai
License: UNKNOWN
Project-URL: Documentation, https://dbnd.readme.io/
Project-URL: Bug-Tracker, https://github.com/databand-ai/dbnd/issues
Project-URL: Source-Code, https://github.com/databand-ai/dbnd
Keywords: orchestration,data,machinelearning
Platform: any
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: Natural Language :: English
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python
Classifier: Programming Language :: Python :: 2
Classifier: Programming Language :: Python :: 2.7
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.4
Classifier: Programming Language :: Python :: 3.5
Classifier: Programming Language :: Python :: 3.6
Classifier: Programming Language :: Python :: 3.7
Classifier: Programming Language :: Python :: Implementation :: CPython
Classifier: Programming Language :: Python :: Implementation :: PyPy
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Description-Content-Type: text/markdown
Requires-Dist: dbnd (==0.35.1)
Requires-Dist: packaging
Requires-Dist: psycopg2 (<2.8,>=2.7.4)
Provides-Extra: airflow
Requires-Dist: WTForms (<2.3.0) ; extra == 'airflow'
Requires-Dist: Werkzeug (<1.0.0,>=0.15.0) ; extra == 'airflow'
Requires-Dist: psycopg2 (<2.8,>=2.7.4) ; extra == 'airflow'
Requires-Dist: apache-airflow (==1.10.9) ; extra == 'airflow'
Provides-Extra: airflow_1_10_10
Requires-Dist: WTForms (<2.3.0) ; extra == 'airflow_1_10_10'
Requires-Dist: Werkzeug (<1.0.0,>=0.15.0) ; extra == 'airflow_1_10_10'
Requires-Dist: psycopg2 (<2.8,>=2.7.4) ; extra == 'airflow_1_10_10'
Requires-Dist: apache-airflow (==1.10.10) ; extra == 'airflow_1_10_10'
Provides-Extra: airflow_1_10_11
Requires-Dist: WTForms (<2.3.0) ; extra == 'airflow_1_10_11'
Requires-Dist: Werkzeug (<1.0.0,>=0.15.0) ; extra == 'airflow_1_10_11'
Requires-Dist: psycopg2 (<2.8,>=2.7.4) ; extra == 'airflow_1_10_11'
Requires-Dist: apache-airflow (==1.10.11) ; extra == 'airflow_1_10_11'
Provides-Extra: airflow_1_10_12
Requires-Dist: WTForms (<2.3.0) ; extra == 'airflow_1_10_12'
Requires-Dist: Werkzeug (<1.0.0,>=0.15.0) ; extra == 'airflow_1_10_12'
Requires-Dist: psycopg2 (<2.8,>=2.7.4) ; extra == 'airflow_1_10_12'
Requires-Dist: apache-airflow (==1.10.12) ; extra == 'airflow_1_10_12'
Provides-Extra: airflow_1_10_7
Requires-Dist: WTForms (<2.3.0) ; extra == 'airflow_1_10_7'
Requires-Dist: Werkzeug (<1.0.0,>=0.15.0) ; extra == 'airflow_1_10_7'
Requires-Dist: psycopg2 (<2.8,>=2.7.4) ; extra == 'airflow_1_10_7'
Requires-Dist: apache-airflow (==1.10.7) ; extra == 'airflow_1_10_7'
Provides-Extra: airflow_1_10_8
Requires-Dist: WTForms (<2.3.0) ; extra == 'airflow_1_10_8'
Requires-Dist: Werkzeug (<1.0.0,>=0.15.0) ; extra == 'airflow_1_10_8'
Requires-Dist: psycopg2 (<2.8,>=2.7.4) ; extra == 'airflow_1_10_8'
Requires-Dist: apache-airflow (==1.10.8) ; extra == 'airflow_1_10_8'
Provides-Extra: airflow_1_10_9
Requires-Dist: WTForms (<2.3.0) ; extra == 'airflow_1_10_9'
Requires-Dist: Werkzeug (<1.0.0,>=0.15.0) ; extra == 'airflow_1_10_9'
Requires-Dist: psycopg2 (<2.8,>=2.7.4) ; extra == 'airflow_1_10_9'
Requires-Dist: apache-airflow (==1.10.9) ; extra == 'airflow_1_10_9'
Provides-Extra: tests
Requires-Dist: pandas (<2.0.0,>=0.17.1) ; extra == 'tests'
Requires-Dist: azure-storage-blob ; extra == 'tests'
Requires-Dist: httplib2 (>=0.9.2) ; extra == 'tests'
Requires-Dist: boto3 (<=1.15.18) ; extra == 'tests'
Requires-Dist: s3fs ; extra == 'tests'
Requires-Dist: google-api-python-client (<2.0.0dev,>=1.6.0) ; extra == 'tests'
Requires-Dist: google-auth (<2.0.0dev,>=1.0.0) ; extra == 'tests'
Requires-Dist: google-auth-httplib2 (>=0.0.1) ; extra == 'tests'
Requires-Dist: google-cloud-container (>=0.1.1) ; extra == 'tests'
Requires-Dist: PyOpenSSL ; extra == 'tests'
Requires-Dist: pandas-gbq ; extra == 'tests'
Requires-Dist: docker (~=3.0) ; extra == 'tests'
Requires-Dist: kubernetes (==9.0.0) ; extra == 'tests'
Requires-Dist: cryptography (>=2.0.0) ; extra == 'tests'
Requires-Dist: WTForms (<2.3.0) ; extra == 'tests'
Requires-Dist: dbnd-test-scenarios (==0.35.1) ; extra == 'tests'

# Dbnd Airflow Operator

This plugin was written to provide an explicit way of declaratively passing messages between two airflow operators.

This plugin was inspired by [AIP-31](https://cwiki.apache.org/confluence/display/AIRFLOW/AIP-31%3A+Airflow+functional+DAG+definition).
Essentially, this plugin connects between dbnd's implementation of tasks and pipelines to airflow operators.

This implementation uses XCom communication and XCom templates to transfer said messages.
This plugin is fully functional, however as soon as AIP-31 is implemented it will support all edge-cases.

Fully tested on airflow 1.10.X.

# Code Example

Here is an example of how we achieve our goal:

```python
import logging
from typing import Tuple
from datetime import timedelta, datetime
from airflow import DAG
from airflow.utils.dates import days_ago
from airflow.operators.python_operator import PythonOperator
from dbnd import task

# Define arguments that we will pass to our DAG
default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "start_date": days_ago(2),
    "retries": 1,
    "retry_delay": timedelta(seconds=10),
}
@task
def my_task(p_int=3, p_str="check", p_int_with_default=0) -> str:
    logging.info("I am running")
    return "success"


@task
def my_multiple_outputs(p_str="some_string") -> Tuple[int, str]:
    return (1, p_str + "_extra_postfix")


def some_python_function(input_path, output_path):
    logging.error("I am running")
    input_value = open(input_path, "r").read()
    with open(output_path, "w") as output_file:
        output_file.write(input_value)
        output_file.write("\n\n")
        output_file.write(str(datetime.now().strftime("%Y-%m-%dT%H:%M:%S")))
    return "success"

# Define DAG context
with DAG(dag_id="dbnd_operators", default_args=default_args) as dag_operators:
    # t1, t2 and t3 are examples of tasks created by instantiating operators
    # All tasks and operators created under this DAG context will be collected as a part of this DAG
    t1 = my_task(2)
    t2, t3 = my_multiple_outputs(t1)
    python_op = PythonOperator(
        task_id="some_python_function",
        python_callable=some_python_function,
        op_kwargs={"input_path": t3, "output_path": "/tmp/output.txt"},
    )
    """
    t3.op describes the operator used to execute my_multiple_outputs
    This call defines the some_python_function task's operator as dependent upon t3's operator
    """
    python_op.set_upstream(t3.op)
```

As you can see, messages are passed explicitly between all three tasks:

-   t1, the result of the first task is passed to the next task my_multiple_outputs
-   t2 and t3 represent the results of my_multiple_outputs
-   some_python_function is wrapped with an operator
-   The new python operator is defined as dependent upon t3's execution (downstream) - explicitly.

> Note: If you run a function marked with the `@task` decorator without a DAG context, and without using the dbnd
> library to run it - it will execute absolutely normally!

Using this method to pass arguments between tasks not only improves developer user-experience, but also allows
for pipeline execution support for many use-cases. It does not break currently existing DAGs.

# Using dbnd_config

Let's look at the example again, but change the default_args defined at the very top:

```python
default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "start_date": days_ago(2),
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
    'dbnd_config': {
        "my_task.p_int_with_default": 4
    }
}
```

Added a new key-value pair to the arguments called `dbnd_config`

`dbnd_config` is expected to define a dictionary of configuration settings that you can pass to your tasks. For example,
the `dbnd_config` in this code section defines that the int parameter `p_int_with_default` passed to my_task will be
overridden and changed to `4` from the default value `0`.

To see further possibilities of changing configuration settings, see our [documentation](https://dbnd.readme.io/)


