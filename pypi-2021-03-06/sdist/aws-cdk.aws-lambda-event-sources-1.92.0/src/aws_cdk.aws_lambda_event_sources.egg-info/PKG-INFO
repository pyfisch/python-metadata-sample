Metadata-Version: 2.1
Name: aws-cdk.aws-lambda-event-sources
Version: 1.92.0
Summary: Event sources for AWS Lambda
Home-page: https://github.com/aws/aws-cdk
Author: Amazon Web Services
License: Apache-2.0
Project-URL: Source, https://github.com/aws/aws-cdk.git
Description: # AWS Lambda Event Sources
        
        <!--BEGIN STABILITY BANNER-->---
        
        
        ![cdk-constructs: Stable](https://img.shields.io/badge/cdk--constructs-stable-success.svg?style=for-the-badge)
        
        ---
        <!--END STABILITY BANNER-->
        
        An event source mapping is an AWS Lambda resource that reads from an event source and invokes a Lambda function.
        You can use event source mappings to process items from a stream or queue in services that don't invoke Lambda
        functions directly. Lambda provides event source mappings for the following services. Read more about lambda
        event sources [here](https://docs.aws.amazon.com/lambda/latest/dg/invocation-eventsourcemapping.html).
        
        This module includes classes that allow using various AWS services as event
        sources for AWS Lambda via the high-level `lambda.addEventSource(source)` API.
        
        NOTE: In most cases, it is also possible to use the resource APIs to invoke an
        AWS Lambda function. This library provides a uniform API for all Lambda event
        sources regardless of the underlying mechanism they use.
        
        The following code sets up a lambda function with an SQS queue event source -
        
        ```python
        # Example automatically generated without compilation. See https://github.com/aws/jsii/issues/826
        fn = lambda_.Function(self, "MyFunction")
        
        queue = sqs.Queue(self, "MyQueue")
        event_source = fn.add_event_source(SqsEventSource(queue))
        
        event_source_id = event_source.event_source_id
        ```
        
        The `eventSourceId` property contains the event source id. This will be a
        [token](https://docs.aws.amazon.com/cdk/latest/guide/tokens.html) that will resolve to the final value at the time of
        deployment.
        
        ## SQS
        
        Amazon Simple Queue Service (Amazon SQS) allows you to build asynchronous
        workflows. For more information about Amazon SQS, see Amazon Simple Queue
        Service. You can configure AWS Lambda to poll for these messages as they arrive
        and then pass the event to a Lambda function invocation. To view a sample event,
        see [Amazon SQS Event](https://docs.aws.amazon.com/lambda/latest/dg/eventsources.html#eventsources-sqs).
        
        To set up Amazon Simple Queue Service as an event source for AWS Lambda, you
        first create or update an Amazon SQS queue and select custom values for the
        queue parameters. The following parameters will impact Amazon SQS's polling
        behavior:
        
        * **visibilityTimeout**: May impact the period between retries.
        * **receiveMessageWaitTime**: Will determine [long
          poll](https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-long-polling.html)
          duration. The default value is 20 seconds.
        * **enabled**: If the SQS event source mapping should be enabled. The default is true.
        
        ```python
        # Example automatically generated without compilation. See https://github.com/aws/jsii/issues/826
        import aws_cdk.aws_sqs as sqs
        from aws_cdk.aws_lambda_event_sources import SqsEventSource
        from aws_cdk.core import Duration
        
        queue = sqs.Queue(self, "MyQueue",
            visibility_timeout=Duration.seconds(30), # default,
            receive_message_wait_time=Duration.seconds(20)
        )
        
        lambda_.add_event_source(SqsEventSource(queue,
            batch_size=10
        ))
        ```
        
        ## S3
        
        You can write Lambda functions to process S3 bucket events, such as the
        object-created or object-deleted events. For example, when a user uploads a
        photo to a bucket, you might want Amazon S3 to invoke your Lambda function so
        that it reads the image and creates a thumbnail for the photo.
        
        You can use the bucket notification configuration feature in Amazon S3 to
        configure the event source mapping, identifying the bucket events that you want
        Amazon S3 to publish and which Lambda function to invoke.
        
        ```python
        # Example automatically generated without compilation. See https://github.com/aws/jsii/issues/826
        import aws_cdk.aws_s3 as s3
        from aws_cdk.aws_lambda_event_sources import S3EventSource
        
        bucket = s3.Bucket(...)
        
        lambda_.add_event_source(S3EventSource(bucket,
            events=[s3.EventType.OBJECT_CREATED, s3.EventType.OBJECT_REMOVED],
            filters=[NotificationKeyFilter(prefix="subdir/")]
        ))
        ```
        
        ## SNS
        
        You can write Lambda functions to process Amazon Simple Notification Service
        notifications. When a message is published to an Amazon SNS topic, the service
        can invoke your Lambda function by passing the message payload as a parameter.
        Your Lambda function code can then process the event, for example publish the
        message to other Amazon SNS topics, or send the message to other AWS services.
        
        This also enables you to trigger a Lambda function in response to Amazon
        CloudWatch alarms and other AWS services that use Amazon SNS.
        
        For an example event, see [Appendix: Message and JSON
        Formats](https://docs.aws.amazon.com/sns/latest/dg/json-formats.html) and
        [Amazon SNS Sample
        Event](https://docs.aws.amazon.com/lambda/latest/dg/eventsources.html#eventsources-sns).
        For an example use case, see [Using AWS Lambda with Amazon SNS from Different
        Accounts](https://docs.aws.amazon.com/lambda/latest/dg/with-sns.html).
        
        ```python
        # Example automatically generated without compilation. See https://github.com/aws/jsii/issues/826
        import aws_cdk.aws_sns as sns
        from aws_cdk.aws_lambda_event_sources import SnsEventSource
        
        topic = sns.Topic(...)
        dead_letter_queue = sqs.Queue(self, "deadLetterQueue")
        
        lambda_.add_event_source(SnsEventSource(topic,
            filter_policy={...},
            dead_letter_queue=dead_letter_queue
        ))
        ```
        
        When a user calls the SNS Publish API on a topic that your Lambda function is
        subscribed to, Amazon SNS will call Lambda to invoke your function
        asynchronously. Lambda will then return a delivery status. If there was an error
        calling Lambda, Amazon SNS will retry invoking the Lambda function up to three
        times. After three tries, if Amazon SNS still could not successfully invoke the
        Lambda function, then Amazon SNS will send a delivery status failure message to
        CloudWatch.
        
        ## DynamoDB Streams
        
        You can write Lambda functions to process change events from a DynamoDB Table. An event is emitted to a DynamoDB stream (if configured) whenever a write (Put, Delete, Update)
        operation is performed against the table. See [Using AWS Lambda with Amazon DynamoDB](https://docs.aws.amazon.com/lambda/latest/dg/with-ddb.html) for more information about configuring Lambda function event sources with DynamoDB.
        
        To process events with a Lambda function, first create or update a DynamoDB table and enable a `stream` specification. Then, create a `DynamoEventSource`
        and add it to your Lambda function. The following parameters will impact Amazon DynamoDB's polling behavior:
        
        * **batchSize**: Determines how many records are buffered before invoking your lambda function - could impact your function's memory usage (if too high) and ability to keep up with incoming data velocity (if too low).
        * **bisectBatchOnError**: If a batch encounters an error, this will cause the batch to be split in two and have each new smaller batch retried, allowing the records in error to be isolated.
        * **maxBatchingWindow**: The maximum amount of time to gather records before invoking the lambda. This increases the likelihood of a full batch at the cost of delayed processing.
        * **maxRecordAge**: The maximum age of a record that will be sent to the function for processing. Records that exceed the max age will be treated as failures.
        * **onFailure**: In the event a record fails after all retries or if the record age has exceeded the configured value, the record will be sent to SQS queue or SNS topic that is specified here
        * **parallelizationFactor**: The number of batches to concurrently process on each shard.
        * **retryAttempts**: The maximum number of times a record should be retried in the event of failure.
        * **startingPosition**: Will determine where to being consumption, either at the most recent ('LATEST') record or the oldest record ('TRIM_HORIZON'). 'TRIM_HORIZON' will ensure you process all available data, while 'LATEST' will ignore all records that arrived prior to attaching the event source.
        * **enabled**: If the DynamoDB Streams event source mapping should be enabled. The default is true.
        
        ```python
        # Example automatically generated without compilation. See https://github.com/aws/jsii/issues/826
        import aws_cdk.aws_dynamodb as dynamodb
        import aws_cdk.aws_lambda as lambda_
        import aws_cdk.aws_sqs as sqs
        from aws_cdk.aws_lambda_event_sources import DynamoEventSource, SqsDlq
        
        table = dynamodb.Table(...,
            partition_key=, ...,
            stream=dynamodb.StreamViewType.NEW_IMAGE
        )
        
        dead_letter_queue = sqs.Queue(self, "deadLetterQueue")def ():
            passlambda_.Function(...)
        def ():
            passadd_event_source(DynamoEventSource(table,
            starting_position=lambda_.StartingPosition.TRIM_HORIZON,
            batch_size=5,
            bisect_batch_on_error=True,
            on_failure=SqsDlq(dead_letter_queue),
            retry_attempts=10
        ))
        ```
        
        ## Kinesis
        
        You can write Lambda functions to process streaming data in Amazon Kinesis Streams. For more information about Amazon Kinesis, see [Amazon Kinesis
        Service](https://aws.amazon.com/kinesis/data-streams/). To learn more about configuring Lambda function event sources with kinesis and view a sample event,
        see [Amazon Kinesis Event](https://docs.aws.amazon.com/lambda/latest/dg/with-kinesis.html).
        
        To set up Amazon Kinesis as an event source for AWS Lambda, you
        first create or update an Amazon Kinesis stream and select custom values for the
        event source parameters. The following parameters will impact Amazon Kinesis's polling
        behavior:
        
        * **batchSize**: Determines how many records are buffered before invoking your lambda function - could impact your function's memory usage (if too high) and ability to keep up with incoming data velocity (if too low).
        * **bisectBatchOnError**: If a batch encounters an error, this will cause the batch to be split in two and have each new smaller batch retried, allowing the records in error to be isolated.
        * **maxBatchingWindow**: The maximum amount of time to gather records before invoking the lambda. This increases the likelihood of a full batch at the cost of possibly delaying processing.
        * **maxRecordAge**: The maximum age of a record that will be sent to the function for processing. Records that exceed the max age will be treated as failures.
        * **onFailure**: In the event a record fails and consumes all retries, the record will be sent to SQS queue or SNS topic that is specified here
        * **parallelizationFactor**: The number of batches to concurrently process on each shard.
        * **retryAttempts**: The maximum number of times a record should be retried in the event of failure.
        * **startingPosition**: Will determine where to being consumption, either at the most recent ('LATEST') record or the oldest record ('TRIM_HORIZON'). 'TRIM_HORIZON' will ensure you process all available data, while 'LATEST' will ignore all records that arrived prior to attaching the event source.
        * **enabled**: If the DynamoDB Streams event source mapping should be enabled. The default is true.
        
        ```python
        # Example automatically generated without compilation. See https://github.com/aws/jsii/issues/826
        import aws_cdk.aws_lambda as lambda_
        import aws_cdk.aws_kinesis as kinesis
        from aws_cdk.aws_lambda_event_sources import KinesisEventSource
        
        stream = kinesis.Stream(self, "MyStream")
        
        my_function.add_event_source(KinesisEventSource(stream,
            batch_size=100, # default
            starting_position=lambda_.StartingPosition.TRIM_HORIZON
        ))
        ```
        
        ## Roadmap
        
        Eventually, this module will support all the event sources described under
        [Supported Event
        Sources](https://docs.aws.amazon.com/lambda/latest/dg/invoking-lambda-function.html)
        in the AWS Lambda Developer Guide.
        
Platform: UNKNOWN
Classifier: Intended Audience :: Developers
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: JavaScript
Classifier: Programming Language :: Python :: 3 :: Only
Classifier: Programming Language :: Python :: 3.6
Classifier: Programming Language :: Python :: 3.7
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Typing :: Typed
Classifier: Development Status :: 5 - Production/Stable
Classifier: License :: OSI Approved
Classifier: Framework :: AWS CDK
Classifier: Framework :: AWS CDK :: 1
Requires-Python: >=3.6
Description-Content-Type: text/markdown
